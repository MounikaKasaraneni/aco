<!DOCTYPE html>
<html lang="en-US">
<head>
<title>ACO Project</title>
<style>
body {
	background-color: rgb(190, 228, 255);
	}
h1   {
	color: black; 
	font-size:30px; 
	font-family:verdana; 
	text-align:center;
	}
h2   {
	color: black; 
	font-size:20px; 
	font-family:verdana; 
	text-align:center;
	}
p    {
	color: black; 
	font-family:verdana; 
	background-color:white;
	}
p {
    border: 1px solid powderblue;
	}
p {
    border: 1px solid powderblue;
    padding: 30px;
	}
p {
    border: 1px solid powderblue;
    margin: 50px;
	}
</style>
</head>
<body>
<img src="https://www.terminix.com/static-srvm/trmx/blog-images/red-ants-marching-main.jpg" style="align:middle; width:1250px; height:300px;">
<h1>Subset selection using ACO Algorithm</h1>
<p>Ant Colony Optimization (ACO) algorithm, first introduced by Dorigo, is a bionic meta-heuristic algorithm stemming from the finding food process of natural ant colony. 
In our context ACO is used to find feasible paths from the graphs. In this problem we are using ACO to find the best feature subset from all the features. </p>
<a href="#ACO">ACO Algorithm</a>
<br>
</br>
<a href="#mlibs">Mlibs algorithms to calculate accuracies</a>
<br>
</br>
<a href="#accuracy">Accuracy</a>
<br>
</br>
<a href="#deliverables">Deliverables</a>
<br>
</br>
<a href="#output">Output of ACO Algorithm</a>
<h2 id="ACO">ACO Algorithm</h2>
<p><b>Correlation Matrix</b><br>
import org.apache.hadoop.fs.Path;<br>
import org.apache.hadoop.io.IntWritable;<br>
import org.apache.hadoop.io.*;<br>
import org.apache.hadoop.conf.*;<br>
import org.apache.hadoop.mapreduce.Job;<br>
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;<br>
import org.apache.hadoop.io.Text;<br>
import org.apache.hadoop.mapreduce.Mapper;<br>
import org.apache.hadoop.mapreduce.Reducer;<br>
import org.apache.hadoop.io.WritableComparable;<br>
import java.io.IOException;<br>
import java.util.*;<br>
import org.apache.hadoop.mapred.*;<br>
import org.apache.commons.math3.linear.Array2DRowRealMatrix;<br>
import org.apache.commons.math3.linear.RealMatrix;<br>
import org.apache.commons.math3.stat.correlation.PearsonsCorrelation;<br>
import org.apache.hadoop.io.DoubleWritable;<br>
import org.apache.commons.math3.exception.MathIllegalNumberException;<br>




public class corr{<br>
	
	
	//****************ARRAYWRITABLE*********************************************************<br>
	public static class IntArrayWritable extends ArrayWritable {<br>
		public IntArrayWritable() {<br>
			super(IntWritable.class);<br>
		}<br>
	}<br>
	public class TwoDArrayWritables extends TwoDArrayWritable<br>
	{<br>
		public TwoDArrayWritables() {<br>
			super(DoubleWritable.class);<br>
		}<br>
		public TwoDArrayWritables(Class valueClass) {<br>
			super(valueClass);<br>
        // TODO Auto-generated constructor stub<br>
		}<br>
	}<br>
	
	
	//*********************MAPPER**************************************************************<br>
	public static class MapDischarges extends Mapper<Object, Text,IntWritable, TwoDArrayWritable ><br>
	{	<br>
		ArrayList<ArrayList<Double>> array1 = new ArrayList<ArrayList<Double>>();<br>
		ArrayList<Double> subList = new ArrayList<Double>();<br>
		IntArrayWritable outputArray = new IntArrayWritable();<br>
		TwoDArrayWritable array = new TwoDArrayWritable (DoubleWritable.class);<br>
			
		public void map(Object key, Text value, Context context) throws IOException, InterruptedException<br>
		{<br>
			String line = value.toString();<br>
			String[] words = line.split(",");<br>
			
			//Reading the input and storing in a list<br>
			for(int j=0;j<words.length;j++)<br>
			{<br>
				subList.add(Double.parseDouble(words[j]));<br>
			}<br>
			array1.add(subList);<br>
			
			int columns= subList.size();<br>
			int rows= array1.size();<br>
			double [][] inputMatrix = new double [rows][columns];<br>
			IntWritable pair= new IntWritable(1);<br>
			
			ArrayList<Double> subList2 = new ArrayList<Double>();<br>
			//Storing the input in a matrix<br>
			for(int i=0;i<rows;i++)<br>
			{<br>
				subList2 = array1.get(i);<br>
				for(int j=0;j<columns;j++)<br>
				{<br>
					inputMatrix[i][j]= subList2.get(j);<br>
					
				}<br>
			}<br>
			
			// computing pearson correlation for input matrix<br>
			PearsonsCorrelation pc = new PearsonsCorrelation();<br>
			Array2DRowRealMatrix input =new Array2DRowRealMatrix(inputMatrix);<br>
			RealMatrix corr;<br>
			corr  = pc.computeCorrelationMatrix(input);<br>
			double correlationArray[][] = corr.getData();<br>
			
			DoubleWritable [][]sendingArray = new DoubleWritable[columns][columns];<br>
			for(int i=0;i<columns;i++)<br>
			{<br>
				for(int j=0;j<columns;j++)<br>
				{<br>
					sendingArray[i][j]= new DoubleWritable(correlationArray[i][j]);<br>
					
				}<br>
			}<br>
			array.set(sendingArray);<br>
			context.write(pair, array);<br>
			
		
		}<br>
	}<br>
	
	// *********************************REDUCER CLASS ***************************************************************<br>
	public static class ReduceDischarges extends Reducer<IntWritable, TwoDArrayWritable ,IntWritable, TwoDArrayWritable ><br>
	{<br>
	
		public void reduce(IntWritable key, TwoDArrayWritable values, Context context)throws IOException, InterruptedException<br>
		{<br>
			context.write(key, values);<br>
		}<br>
	}<br>
	
	//************************************DRIVER CLASS *************************************************************************<br>
	public static void main(String [] args) throws Exception<br>
	{<br>
		Configuration conf = new Configuration();<br>
		Job job = Job.getInstance(conf, "correlation array");<br>
        job.setJarByClass(corr.class);<br>
		//setting the mapper , reducer class and output key and value class <br>
        job.setMapperClass(MapDischarges.class);<br>
        job.setReducerClass(ReduceDischarges.class);<br>
        job.setOutputKeyClass(IntWritable.class);<br>
        job.setOutputValueClass(TwoDArrayWritable.class);<br>
		job.setMapOutputKeyClass(IntWritable.class);<br>
		job.setMapOutputValueClass(TwoDArrayWritable.class);<br>
		// setting input and output paths<br>
        FileInputFormat.addInputPath(job, new Path(args[0]));<br>
        FileOutputFormat.setOutputPath(job, new Path(args[1]));<br>
        System.exit(job.waitForCompletion(true) ? 0 : 1);<br>
	}<br>
	
	
}<br>
</br>
</p>
<p><b>ACO Algorithm</b><br>
import org.apache.hadoop.fs.Path;<br>
import org.apache.hadoop.io.IntWritable;<br>
import org.apache.hadoop.io.*;<br>
import org.apache.hadoop.conf.*;<br>
import org.apache.hadoop.mapreduce.Job;<br>
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;<br>
import org.apache.hadoop.io.Text;<br>
import org.apache.hadoop.mapreduce.Mapper;<br>
import org.apache.hadoop.mapreduce.Reducer;<br>
import org.apache.hadoop.io.WritableComparable;<br>
import java.io.IOException;<br>
import java.util.*;<br>
import org.apache.hadoop.mapred.*;<br>
import org.apache.commons.math3.linear.Array2DRowRealMatrix;<br>
import org.apache.commons.math3.linear.RealMatrix;<br>
import org.apache.commons.math3.stat.correlation.PearsonsCorrelation;<br>
import org.apache.hadoop.io.DoubleWritable;<br>
import java.io.*;<br>
import org.apache.hadoop.io.ArrayWritable;<br>
import org.apache.hadoop.io.Writable;<br>




public class acomr{<br>
	
	
	//****************ARRAYWRITABLE*********************************************************<br>

	public static class ArrayWritables extends ArrayWritable {<br>
		public ArrayWritables() {<br>
			super(ArrayWritable.class);<br>
		}<br>
		public ArrayWritables(Class valueClass) {<br>
			super(valueClass);<br>
        // TODO Auto-generated constructor stub<br>
		}<br>
		
	}<br>
	
	
	public static class IntArrayWritable extends ArrayWritable {<br>

        public IntArrayWritable(Writable[] values) {<br>
                super(IntWritable.class, values);<br>
        }<br>

        public IntArrayWritable() {<br>
                super(IntWritable.class);<br>
        }<br>
        public IntArrayWritable(Class valueClass, Writable[] values) {<br>
                super(IntWritable.class, values);<br>
        }<br>

        public IntArrayWritable(Class valueClass) {<br>
                super(IntWritable.class);<br>
        }<br>

        public IntArrayWritable(String[] strings) {<br>
                super(strings);<br>
        }<br>
		<br>
		@Override<br>
		public String toString() {<br>
			StringBuilder sb = new StringBuilder();<br>
			for (String s : super.toStrings())<br>
			{<br>
				sb.append(s).append(" ");<br>
			}<br>
			return sb.toString();<br>
		}<br>
	}
	<br>
	//*********************MAPPER**************************************************************<br>
	public static class MapDischarges extends Mapper<Object, Text,IntWritable, IntArrayWritable ><br>
	{	<br>
		ArrayList<ArrayList<Double>> array1 = new ArrayList<ArrayList<Double>>();<br>
		ArrayList<Double> subList = new ArrayList<Double>();<br>
		IntArrayWritable outputArray = new IntArrayWritable();<br>
		
		static int destination = 8;<br>
		IntWritable outputKey = new IntWritable();<br>
			int k=0;<br>
		public void map(Object key, Text value, Context context) throws IOException, InterruptedException<br>
		{<br>
			String line = value.toString();<br>
			String[] words = line.split(",");
			<br>
			//Reading the input and storing in a list<br>
			for(int j=0;j<words.length;j++)<br>
			{<br>
				subList.add(Double.parseDouble(words[j]));<br>
			}<br>
			array1.add(subList);
			<br>
		//	int columns= subList.size(); <br>
		int columns =9;<br>
			int rows= array1.size();<br>
			double [][] inputMatrix = new double [rows][columns];<br>
			IntWritable pair= new IntWritable(1);<br>
			
			ArrayList<Double> subList2 = new ArrayList<Double>();<br>
			//Storing the input in a matrix<br>
			for(int i=0;i<rows;i++)<br>
			{<br>
				subList2 = array1.get(i);<br>
				for(int j=0;j<columns;j++)<br>
				{<br>
					inputMatrix[i][j]= subList2.get(j);<br>
					
				}<br>
			}
			<br>
			
			
			//Initializing pheromone<br>
			double pheromone[][] = new double[columns][columns];<br>
			pheromone = initializePheromone(columns);<br>
			                   
			
			//*********GENERATE ANTS, RELEASE ANTS ******<br>		
			int numberOfAnts = columns - 1;<br>
			int numAnts[] = new int[columns-1];<br>
			int antSource[] = new int[numberOfAnts];<br>
			for(int i=0 ; i< numberOfAnts ; i++)<br>
			{<br>
				antSource[i] = i; /*************CHECK : Need to generate ants randomly not serially*/<br>
			}<br>
			
			List<List<Integer>> subset= new ArrayList<List<Integer>> (numberOfAnts);<br>
			List<Integer> setOfFeature = new ArrayList<Integer>();<br>
						<br>
			// Finding the routes for each ant<br>
			for(int i = 0; i < numberOfAnts; i++)  {<br>
				subset.add(new ArrayList<Integer>());<br>
			}<br>
			
		
			
			setOfFeature= getFeatureSubSet(antSource[k], destination, pheromone, inputMatrix, columns);<br>
			IntWritable [] array2 = new IntWritable[setOfFeature.size()];<br>
			for (int k1 = 0; k1 < setOfFeature.size(); k1++) {<br>
					array2[k1] = new IntWritable(setOfFeature.get(k1));<br>
				}<br>
		
			outputArray.set(array2);<br>
				outputKey.set(k);<br>
			context.write(outputKey, outputArray);	<br>
			k++;<br>
			
		}<br>
		
		
		public static double[][] initializePheromone(int featureCount)<br>
		{<br>
			double pheromone[][] = new double[featureCount][featureCount];<br>
			for(int i=0;i<featureCount;i++)<br>
			{<br>
				for(int j=0;j<featureCount;j++)<br>
				{<br>
					pheromone[i][j] = 1;<br>
				}<br>
			}<br>
			return pheromone;<br>
		}<br>
	
		public static int getNextNode(int sourceNode, List<Integer> remainingNodes, double pheromone[][], double[][] corrArray)<br>
		{<br>
			double lower=0.0,upper=0.0;<br>
			int nextNode;<br>
			double eta=0.0,eta1=0.0;<br>
			double toque=0.0,toque1=0.0;<br>
			double probabilities[] = new double[remainingNodes.size()];<br>
			double max;<br>
			
			/* this loop to find probabilities for the nodes not traversed from source node */<br>
			for(int i=0; i<remainingNodes.size();i++)<br>
			{<br>
				//eta=corrArray[sourceNode][remainingNodes.get(i)];//probabilities[i] = calculateProb(sourceNode, remainingNodes,pheromone, corrArray);<br>
				eta=1;<br>
				toque=pheromone[sourceNode][remainingNodes.get(i)];<br>
				upper=eta*toque;<br>
				for(int j=0; j<remainingNodes.size();j++)<br>
				{<br>
					//eta1=corrArray[sourceNode][remainingNodes.get(j)];//probabilities[i] = calculateProb(sourceNode, remainingNodes,pheromone, corrArray);<br>
					eta1=1;<br>
					toque1=pheromone[sourceNode][remainingNodes.get(j)];<br>
					lower+=eta1*toque1;<br>
				}<br>
				probabilities[i]=upper/lower;<br>
			}<br>
		
			max = 0.0;<br>
			nextNode = remainingNodes.get(0);<br>
		
			/*this loop to check the node which has maximum probability */<br>
			for(int i=0; i< probabilities.length; i++)<br>
			{<br>
				if(max < probabilities[i])<br>
				{<br>
					max = probabilities[i];<br>
					nextNode = remainingNodes.get(i);<br>
				}<br>
			}<br>
			return nextNode;<br>
		}<br>
	
	
		//***********GET SUBSET OF FEATURES FOR ONE ANT **********<br>
	
		public static List<Integer> getFeatureSubSet(int source,int dest,  double pheromone[][],double[][]inputMatrix, int columns)<br>
		{<br>
			 List<Integer>featureA = new ArrayList<Integer>();<br>
			for(int i=0;i<columns;i++)<br>
			{<br>
				featureA.add(i);<br>
			}<br>
			List<Integer> traversedFeatures = new ArrayList<Integer>();// to keep track of already traversed features<br>
			List<Integer> remainingFeatures = new ArrayList<Integer>(featureA.size());// to keep track of possible remaining features to traverse<br>
			List<Integer> subset = new ArrayList<Integer>();<br>
			int nextFeature;<br>
		<br>
			remainingFeatures.addAll(featureA);<br>
			remainingFeatures.remove(source);<br>
			traversedFeatures.add(source);<br>
		
			//Loop to find next node to traverse till the destination is reached<br>
			do{<br>
				nextFeature = getNextNode(source, remainingFeatures, pheromone, inputMatrix);<br>
				traversedFeatures.add(nextFeature);<br>
			
				if(nextFeature == dest)<br>
				{<br>
					subset = traversedFeatures;<br>
					break;<br>
				}<br>
				else <br>
				{<br>
					for(Integer x: remainingFeatures)<br>
					{<br>
						if(x == nextFeature)<br>
						{<br>
							remainingFeatures.remove(x);<br>
							break;<br>
						}	<br>
					}<br>
					source = nextFeature;<br>
				}<br>
			}while(nextFeature!= dest);<br>
			return subset;<br>
		}<br>	
	}<br>


	// *********************************REDUCER CLASS ***************************************************************<br>
	public static class ReduceDischarges extends Reducer<IntWritable, IntArrayWritable ,IntWritable, IntArrayWritable ><br>
	{<br>
		public void reduce(IntWritable key, IntArrayWritable values, Context context)throws Exception, InterruptedException<br>
		{<br>
			context.write(key, values);<br>
		}<br>
	}<br>
	<br>
	//************************************DRIVER CLASS *************************************************************************<br>
	public static void main(String [] args) throws Exception<br>
	{<br>
		Configuration conf = new Configuration();<br>
		Job job = Job.getInstance(conf, "mounika_kasaraneni_Program1");<br>
        job.setJarByClass(acomr.class);<br>
		//setting the mapper , reducer class and output key and value class <br>
        job.setMapperClass(MapDischarges.class);<br>
        job.setReducerClass(ReduceDischarges.class);<br>
        job.setOutputKeyClass(IntWritable.class);<br>
        job.setOutputValueClass(IntArrayWritable.class);<br>
		job.setMapOutputKeyClass(IntWritable.class);<br>
		job.setMapOutputValueClass(IntArrayWritable.class);<br>
		// setting input and output paths<br>
        FileInputFormat.addInputPath(job, new Path(args[0]));<br>
        FileOutputFormat.setOutputPath(job, new Path(args[1]));<br>
        System.exit(job.waitForCompletion(true) ? 0 : 1);<br>
	}<br>
	
	
}<br>
</br>
</p>
<br><a href="homeaco.html" title=“top”>Go to top</a></br>
<h2 id="mlibs">Mlibs algorithms to calculate accuracies</h2>
<p><b>Libsvm</b> <br>
# python program to get the data into libsvm format<br>
import sys<br>
import csv<br>
from collections import defaultdict<br>

def construct_line( label, line ):<br>
   new_line = []<br>
   if float( label ) == 0.0:<br>
      label = "0"<br>
   new_line.append( label )<br>

   for i, item in enumerate( line ):<br>
      if item == '' or float( item ) == 0.0:<br>
         continue<br>
      new_item = "%s:%s" % ( i + 1, item )<br>
      new_line.append( new_item )<br>
   new_line = " ".join( new_line )<br>
   new_line += "\n"<br>
   return new_line<br>
# reading input and output data<br>

input_file = "revised.csv"<br>
output_file = "format.txt"<br>
# getting label as 0<br>
try:<br>
   label_index = int(0)<br>
except IndexError:<br>
   label_index = 0<br>

try:<br>
   skip_headers = 0<br>
except IndexError:<br>
   skip_headers = 0<br>

i = open( input_file, 'r' )<br>
o = open( output_file, 'w' )<br>

reader = csv.reader( i )<br>

if skip_headers:<br>
   headers = reader.next()<br>

for line in reader:<br>
   if label_index == -1:<br>
      label = '1'<br>
   else:<br>

      label = line.pop( label_index )<br>

   new_line = construct_line( label, line )<br>
   o.write( new_line )<br>
</br>
</p>
<p><b>Decision Tree</b> <br>
#importing the packages that are required <br>
from pyspark import SparkContext<br>
from pyspark.mllib.tree import DecisionTree, DecisionTreeModel<br>
from pyspark.mllib.util import MLUtils<br>

#Creating the spark context<br>
sc = SparkContext(appName="PythonDecisionTreeRegressionExample")<br>

# training the train data in in libsvm format<br>
trainingData = MLUtils.loadLibSVMFile(sc,"train_data1.txt")<br>
# loading the test data in libsvm format<br>
testData = MLUtils.loadLibSVMFile(sc,"test_data1.txt")<br>
#training the model for Decision trees<br>
model = DecisionTree.trainClassifier(trainingData, numClasses=12999, categoricalFeaturesInfo={},
                                     impurity='entropy', maxDepth=5, maxBins=32)<br>

# Evaluate model on test data and compute Accuracy<br>
predictions = model.predict(testData.map(lambda x: x.features))<br>
labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)<br>
testErr = labelsAndPredictions.filter(
    lambda lp: lp[0] == lp[1]).count() / float(testData.count())<br>
print('Accuracy = %f '%testErr)<br>
</br>
</p>
<p><b>Logistic Regression</b><br>
from __future__ import print_function<br>

# $example on$<br>
from pyspark.ml.classification import LogisticRegression<br>
# $example off$<br>
from pyspark.sql import SparkSession<br>

if __name__ == "__main__":<br>
    spark = SparkSession \<br>
        .builder \<br>
        .appName("MulticlassLogisticRegressionWithElasticNet") \<br>
        .getOrCreate()<br>

    # $example on$<br>
    # Load training data<br>
    training = spark \<br>
        .read \<br>
        .format("libsvm") \<br>
        .load("format.txt")<br>

    lr = LogisticRegression(maxIter=2, regParam=0.1, elasticNetParam=0.1)<br>

    # Fit the model<br>
    lrModel = lr.fit(training)<br>
    trainingSummary = lrModel.summary<br>

    accuracy = trainingSummary.accuracy<br>
    
    print("Accuracy: %s\n"<br>
          % (accuracy*100))<br>
    # $example off$<br>

    file=open("accuracyoutput.txt",'a')<br>
    file1=open("accuracyoutputgraphs.txt",'a')<br>
    file.write(str(accuracy*100))<br>
    file.write("\n")<br>
    file1.write(str(accuracy*100))<br>
    file1.write("\n")<br>
    spark.stop()<br>
</br></p>
<br><a href="homeaco.html" title=“top”>Go to top</a></br>
<h2 id="accuracy">Accuracy</h2>
<p>import java.io.BufferedReader;<br>
import java.io.File;<br>
import java.io.FileNotFoundException;<br>
import java.io.FileReader;<br>
import java.io.IOException;<br>
import java.nio.file.Files;<br>
import java.nio.file.Paths;<br>
import java.util.ArrayList;<br>
import java.util.Arrays;<br>
import java.util.Collections;<br>
import java.util.HashMap;<br>
import java.util.List;<br>
import java.util.stream.Collectors;<br>

public class Accuracies {<br>
static int count=0;<br>
static int i=0;<br>
	public static void main(String[] args) throws IOException {<br>
		 File file = new File("C:\\Users\\ruchishya\\Desktop\\accuracies.txt");<br>
		  BufferedReader br = new BufferedReader(new FileReader(file));<br>
		   String st;<br>
	  List <Integer> newarray=new ArrayList<Integer>();<br>
		System.out.println(count);<br>
		String[] accuracy=new String[4];<br>
		BufferedReader br1 = new BufferedReader(new FileReader(file));<br>
		String st1;<br>
		HashMap< Double,Integer> map = new HashMap<>();  <br> 
		  while ((st1 = br1.readLine()) != null)<br>
		  {<br>
			 
		    accuracy[count]=st1;<br>
		    i++;<br>
		    // Store count as key and accuracy as value in map<br>
		    map.put( Double.parseDouble(st1),count);<br>
		    count++;<br>
		  }<br>
		for(int j=0;j<count;j++)<br>
		{<br>
			System.out.println(accuracy[j]);<br>
		}<br>
		Arrays.sort(accuracy);<br>
		System.out.printf("Modified arr[] : %s",<br>
                Arrays.toString(accuracy));<br>
		int g=accuracy.length-1;<br>
		//System.out.println("\n"+g);<br>
		Double highest = Double.parseDouble(accuracy[g]);<br>
		//System.out.println(highest+"hifhg");<br>
		
		double threshold = 3.0;<br>
		
		double checkAccuracy = highest - threshold;<br>
		
		//System.out.println(checkAccuracy);<br>
		for(int i=0;i<accuracy.length;i++)<br>
		{<br>
		if(Double.parseDouble(accuracy[i])>=checkAccuracy)<br>
		{<br>
			double y=Double.parseDouble(accuracy[i]);<br>
			if(map.containsKey(y))<br>
			{<br>
				int a = map.get(y);<br>
				String line32 = Files.readAllLines(Paths.get("C:\\Users\\ruchishya\\Desktop\\antroutes.txt")).get(a);<br>
				//System.out.println("\n");<br>
				//System.out.println(line32);<br>
				String value[]=line32.split(":");<br>
				//System.out.println(value[1]);<br>
				String num[]=value[1].split(",");<br>
				for(int h=0;h<num.length;h++)<br>
				{<br>
					newarray.add(Integer.parseInt(num[h]));<br>
				}<br>
			}<br>

		}<br>
		}<br>
		List<Integer> finalfeatures= newarray.stream().distinct().collect(Collectors.toList());<br>
		for(int i=0;i<finalfeatures.size();i++){<br>
		    System.out.println(finalfeatures.get(i));<br>
		   
		} <br>
	}<br>

}<br>


/*double y=Double.parseDouble(accuracy[i]);<br>
			if(map.containsKey(y))<br>
			{<br>
				int a = map.get(y);<br>
				String line32 = Files.readAllLines(Paths.get("C:\\Users\\ruchishya\\Desktop\\antroutes.txt")).get(a+1);<br>
			}*/
</br></p>
<br><a href="homeaco.html" title=“top”>Go to top</a></br>
<h2 id="deliverables">Deliverables</h2>
<p>
<b>GITHUB link:</b> <a href="https://github.com/acoproject123/aco" title=“GitHub”>https://github.com/acoproject123/aco</a>
<br>
</br>
</p>
<br><a href="homeaco.html" title=“top”>Go to top</a></br>
<h2 id="output">Output of ACO Algorithm</h2>
<p><b>Average accuracy from Decision tree</b> = 6.13% <br>
<b>Average accuracy from logistic regression</b> = 21.9% <br>
</br></p>
<br><a href="homeaco.html" title=“top”>Go to top</a></br>
</body>
</html>
