from pyspark import SparkContext
# $example on$
from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel
from pyspark.mllib.util import MLUtils
from pyspark.mllib.linalg import Vectors
from pyspark.mllib.regression import LabeledPoint
import numpy as np
from pyspark.mllib.linalg import SparseVector


sc = SparkContext(appName="PythonNaiveBayesExample")

    # $example on$
    # Load and parse the data file.
data = sc.textFile( "train_data.csv").collect()
    #print(np.array(data).tolist())#data=data.map(lambda line: line.split(",")).collect()
label=sc.textFile( "train_labels.csv").map(lambda line: line.split(",")).filter(lambda line: len(line)<=1).map(lambda line: float(line[0])).collect()
    # Split data approximately into training (60%) and test (40%)
 #   training, test = data.randomSplit([0.6, 0.4])
test= sc.textFile( "test_data.csv").map(lambda line: line.split(",")).collect()

testlabel= sc.textFile("test_labels.csv").map(lambda line: line.split(",")).collect()
data_rdd = sc.parallelize(data)
labels_rdd=sc.parallelize(label)
labeled_points_rdd= data_rdd.map(lambda row: row.split(',')).map(lambda seq: LabeledPoint(seq[-1],seq[:-1]))
print(labeled_points_rdd.take(2))
model = NaiveBayes.train(labeled_points_rdd)

    # Make prediction and test accuracy.
predictionAndLabel = testFloat.map(lambda p: (model.predict(p.features), p.LabelFloat))
accuracy = 1.0 * predictionAndLabel.filter(lambda pl: pl[0] == pl[1]).count() / test.count()
print('model accuracy {}'.format(accuracy))
~
